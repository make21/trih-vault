# The Rest Is History Pipeline — Product Requirements

## 1. Goal
Deliver a deterministic, append-only data pipeline that ingests *The Rest Is History* podcast RSS feed, layers in programmatic cleanup and cached LLM enrichments, and publishes stable JSON artefacts for downstream consumers. The new system must minimise LLM token usage, support incremental updates, and remove the legacy "topics" concept entirely.

## 2. Scope
- **In scope**
  - RSS ingestion from `https://feeds.megaphone.fm/GLT4787413333`.
  - Programmatic transforms that normalise episode text, strip boilerplate, and extract structured credits.
  - Deterministic series grouping with derived aggregates (e.g. `yearFrom`, `yearTo`).
  - LLM passes for episodes and series that consume the programmatic fingerprints and reuse cached responses.
  - Composition of public JSON outputs without regenerating historical data unnecessarily.
  - Schema-backed validation, referential integrity checks, and CI ergonomics.
- **Out of scope**
  - Any topic generation, storage, or validation.
  - UI or playback features; this PRD focuses purely on the data pipeline.

## 3. Personas & Needs
| Persona | Need |
| --- | --- |
| Data pipeline maintainer | Run a predictable, idempotent set of scripts locally or in CI without re-enriching unchanged items. |
| Downstream consumer | Read `public/episodes.json` and `public/series.json` with complete metadata, including deterministic episode cleanup and series spans. |
| Cost controller | Keep OpenAI usage low by caching outputs and avoiding re-requests unless fingerprints change. |

## 4. Data Model & Storage
### 4.1 Conventions & Identifiers
- **Canonical IDs**
  - `episodeId` = normalised RSS `<guid>` string. Persist the raw RSS GUID under `source.guid` for traceability.
  - `seriesId` = `toSlug(seriesKey) + "-" + firstPubYYYYMMDD` for the arc (see §22). This ID MUST be stable across reruns and cannot be reused for a different arc.
- **Source mapping** — Every episode retains `source` metadata:
  - `source.guid` (string, required)
  - `source.itunesEpisode` (number \| null, required, mirror the RSS value; null when absent)
  - `source.megaphoneId` (string \| null, optional; populate when present in the feed)
  - `source.enclosureUrl` (string, required)
- **Date handling**
  - `publishedAt` is an ISO 8601 UTC timestamp derived from RSS `pubDate`; present in all layers.
  - `rssLastSeenAt` is an ISO 8601 UTC timestamp representing the last time the RSS entry was observed; stored on raw episodes and propagated through programmatic and public layers.
  - Daily RSS snapshots (`data/source/rss.YYYY-MM-DD.json`) store `fetchedAt` (ISO 8601 UTC) at the file root for provenance.
- **Year semantics**
  - `yearFrom` and `yearTo` are `number \| null` throughout the pipeline. Negative values denote BCE years and must be preserved end-to-end.
  - Replace all prior `"NA"` sentinels with `null`.
  - `yearConfidence` captures precision with enum `"high" | "medium" | "low" | "unknown"`.
- **Series grouping confidence**
  - `seriesGroupingConfidence` enum `"high" | "medium" | "low"` documents how certain the programmatic grouping is.
  - Persist `seriesKeyRaw` when ambiguity remains; confidence defaults to `"low"` in that case.
- **Ordering & determinism**
  - `public/episodes.json` and `public/series.json` are arrays sorted by `publishedAt` ascending, then `episodeId`.
  - All JSON serialisation uses stable key ordering (sorted keys) to minimise diffs.

### 4.2 File Specifications
Explicit field inventories for each artefact:

#### `data/source/rss.YYYY-MM-DD.json`
- Root object with keys:
  - `fetchedAt` — string (ISO 8601 UTC), required.
  - `items` — array of raw RSS item payloads (structure defined by the feed), required; stored for auditing only.

#### `data/episodes-raw.json`
- Append-only array of episode metadata harvested from RSS.
- Fields per entry:

| Field | Type | Required | Notes |
| --- | --- | --- | --- |
| `episodeId` | string | Required | Normalised RSS GUID; canonical ID. |
| `title` | string | Required | Original episode title. |
| `publishedAt` | string | Required | ISO 8601 UTC derived from RSS `pubDate`. |
| `description` | string | Required | Raw HTML or text description from RSS. |
| `audioUrl` | string | Required | Direct media URL. |
| `rssLastSeenAt` | string | Required | ISO 8601 UTC when the item was last seen in RSS. |
| `source` | object | Required | Provenance object (`guid`, `itunesEpisode`, `megaphoneId`, `enclosureUrl`); `itunesEpisode` stored as number \| null. |

#### `data/episodes-programmatic.json`
- Deterministic enrichment stored as an object keyed by `episodeId`.
- Field expectations:
  - `cleanTitle` (plain text version of the episode title after cleanup).
  - `cleanDescriptionMarkdown` (Generated by parsing the source HTML. It must replace `<p>` tags with double newlines (`\n\n`) and `<br>` tags with single newlines (`\n`) to preserve paragraph structure for UI rendering).
  - `cleanDescriptionText` (Derived from `cleanDescriptionMarkdown` by stripping all remaining formatting and collapsing multiple whitespace characters into single spaces. The result must be a clean block of plain text suitable for LLM analysis).
  - *Rationale for two description fields:* This provides maximum flexibility. Downstream web UIs can easily render `cleanDescriptionMarkdown` as HTML with correct paragraph breaks. The `cleanDescriptionText` field provides a simplified, pure-text version optimized for LLM prompts and other machine-reading contexts without the noise of formatting characters.
- Fields per entry:

| Field | Type | Required | Notes |
| --- | --- | --- | --- |
| `episodeId` | string | Required | Mirrors raw `episodeId`. |
| `publishedAt` | string | Required | Carried through from raw layer. |
| `rssLastSeenAt` | string | Required | Copied from raw layer for provenance. |
| `cleanTitle` | string | Required | Plain text version of the episode title after cleanup. |
| `cleanDescriptionMarkdown` | string | Required | Markdown-like synopsis preserving paragraph breaks (double newlines for `<p>`, single newline for `<br>`). |
| `cleanDescriptionText` | string | Required | Formatting-free synopsis derived from `cleanDescriptionMarkdown` with collapsed whitespace. |
| `descriptionBlocks` | string[] | Required | Paragraph-level breakdown used by prompts. |
| `credits` | object | Optional | Keys map to contributor arrays (strings). |
| `fingerprint` | string | Required | See §5 for the algorithm; must reflect the `cleanupVersion`. |
| `cleanupVersion` | number | Required | Current deterministic cleanup version (initially `1`). |
| `derived` | object | Optional | Deterministic facts such as `durationSeconds` (number), `contentWarnings` (string[]), and `yearHints` (object with inferred numeric spans). |
| `part` | number \| null | Optional | Numeric part index; `null` when not applicable. |
| `seriesId` | string \| null | Optional | Stable identifier shared by multi-part arcs; required when `part` is non-null. |
| `seriesKey` | string \| null | Optional | Canonical human-readable key extracted from the title. |
| `seriesKeyRaw` | string \| null | Optional | Original ambiguous key when fallback logic applies. |
| `seriesGroupingConfidence` | string | Required | Enum `"high" | "medium" | "low"`. |
| `yearFrom` | number \| null | Optional | Deterministic span when derivable without LLM. |
| `yearTo` | number \| null | Optional | Deterministic span when derivable without LLM. |
| `yearConfidence` | string | Optional | Enum defined above; reflects programmatic certainty. |

#### `data/series-raw.json`
- Deterministic grouping step represented as an object keyed by `seriesId`.
- Fields per entry:

| Field | Type | Required | Notes |
| --- | --- | --- | --- |
| `seriesId` | string | Required | Stable identifier copied from programmatic episodes. |
| `seriesKey` | string | Optional | Normalised textual key derived from titles. |
| `seriesKeyRaw` | string \| null | Optional | Original ambiguous key when heuristic fallback used. |
| `episodeIds` | string[] | Required | Ordered membership of the series; minimum length 2. |
| `firstPublishedAt` | string | Required | ISO 8601 UTC earliest publication in the arc. |
| `seriesGroupingConfidence` | string | Required | Enum `"high" | "medium" | "low"`. |

#### `data/series-programmatic.json`
- Aggregates computed from raw groupings and programmatic episodes.
- Fields per entry:

| Field | Type | Required | Notes |
| --- | --- | --- | --- |
| `seriesId` | string | Required | Mirrors `data/series-raw.json.seriesId`. |
| `episodeIds` | string[] | Required | Ordered list matching the raw grouping. |
| `fingerprint` | string | Required | See §5 for the algorithm. |
| `memberEpisodeFingerprints` | string[] | Required | Ordered fingerprints from the episode layer. |
| `yearFrom` | number \| null | Required | Minimum member year or null. |
| `yearTo` | number \| null | Required | Maximum member year or null. |
| `yearConfidence` | string | Required | Enum; lowest confidence across members. |
| `seriesTitleFallback` | string | Required | Cleaned `seriesKey` used when LLM title is unavailable. |
| `derived` | object | Optional | Deterministic metadata such as `episodeCount` (number) or `subjectTags` (string[]). |

#### `data/episodes-llm.json`
- Cached LLM enrichments keyed by `${episodeId}:${fingerprint}`.
- Minimal schema (see §12) enforced per entry:

| Field | Type | Required | Notes |
| --- | --- | --- | --- |
| `episodeId` | string | Required | Episode identifier used to construct the cache key. |
| `fingerprint` | string | Required | Programmatic fingerprint associated with the response. |
| `model` | string | Required | LLM model identifier (`gpt-5-nano` primary, fallback lesser model when unavailable). |
| `promptVersion` | string | Required | Semantic version of the prompt template. |
| `createdAt` | string | Required | ISO 8601 UTC timestamp recorded at write. |
| `status` | string | Required | Enum `"ok" | "skipped" | "error"`. |
| `notes` | string \| null | Optional | Human-readable context for skipped/error states. |
| `keyPeople` | string[] | Required | Max 12, unique, hosts removed, whitespace-trimmed. |
| `keyPlaces` | string[] | Required | Max 12, unique, whitespace-trimmed. |
| `keyThemes` | string[] | Required | Between 3 and 8 entries, kebab-case, trimmed. |
| `keyTopics` | object[] | Required | Up to 3 entries referencing the curated registry. Each item = `{ id, label, slug, isPending?, notes? }`. |
| `yearFrom` | number \| null | Required | Numeric year or null. |
| `yearTo` | number \| null | Required | Numeric year or null. |
| `yearConfidence` | string | Required | Enum defined above. |

#### `data/series-llm.json`
- Cached LLM metadata keyed by `${seriesId}:${fingerprint}`.

| Field | Type | Required | Notes |
| --- | --- | --- | --- |
| `seriesId` | string | Required | Identifier shared across series layers. |
| `fingerprint` | string | Required | Mirrors the programmatic fingerprint used when requesting the LLM. |
| `model` | string | Required | LLM model identifier (`gpt-5-nano` primary with fallback lesser model). |
| `promptVersion` | string | Required | Semantic version string. |
| `createdAt` | string | Required | ISO 8601 UTC timestamp. |
| `status` | string | Required | Enum `"ok" | "skipped" | "error"`. |
| `notes` | string \| null | Optional | Context for skipped/error outputs. |
| `seriesTitle` | string \| null | Optional | LLM-provided human title. |
| `narrativeSummary` | string \| null | Optional | Markdown-friendly synopsis. |
| `tonalDescriptors` | string[] | Optional | Unique descriptors when present. |
| `yearFrom` | number \| null | Optional | Harmonised span aligned with episodes. |
| `yearTo` | number \| null | Optional | Harmonised span aligned with episodes. |
| `yearConfidence` | string | Required | Enum defined above. |

#### `public/episodes.json`
- Ordered array combining raw, programmatic, and LLM data per episode, sorted as described.

| Field | Type | Required | Notes |
| --- | --- | --- | --- |
| `id` | string | Required | Mirrors `episodeId`; maintained for consumer continuity. |
| `episodeId` | string | Required | Canonical ID for forward compatibility. |
| `title` | string | Required | Original episode title. |
| `publishedAt` | string | Required | ISO 8601 UTC. |
| `description` | string | Required | Original description. |
| `audioUrl` | string | Required | Direct media URL. |
| `rssLastSeenAt` | string | Required | Provenance timestamp. |
| `itunesEpisode` | number \| null | Optional | Nullable alias of the RSS iTunes episode number. |
| `cleanTitle` | string | Required | Programmatic field. |
| `cleanDescriptionMarkdown` | string | Required | Programmatic field. |
| `cleanDescriptionText` | string | Required | Programmatic field. |
| `descriptionBlocks` | string[] | Required | Programmatic field. |
| `credits` | object | Optional | Programmatic field. |
| `fingerprint` | string | Required | Programmatic fingerprint. |
| `cleanupVersion` | number | Required | Deterministic cleanup version used. |
| `derived` | object | Optional | Programmatic metadata. |
| `part` | number \| null | Optional | Multi-part index. |
| `seriesId` | string \| null | Optional | Present when part populated. |
| `seriesKey` | string \| null | Optional | Canonical key. |
| `seriesKeyRaw` | string \| null | Optional | Ambiguity trace. |
| `seriesGroupingConfidence` | string | Required | Enum. |
| `keyPeople` | string[] | Required | LLM enrichment. |
| `keyPlaces` | string[] | Required | LLM enrichment. |
| `keyThemes` | string[] | Required | LLM enrichment. |
| `keyTopics` | object[] | Optional (v1) | Array of `{ id, label, slug, isPending? }` topic refs sourced from the curated registry. Required once the LLM prompt ships. |
| `yearFrom` | number \| null | Required | LLM-derived or programmatic fallback. |
| `yearTo` | number \| null | Required | LLM-derived or programmatic fallback. |
| `yearConfidence` | string | Required | Highest confidence among sources; prefer LLM. |

#### `public/series.json`
- Ordered array combining raw, programmatic, and LLM data per series, sorted by earliest `publishedAt` then `seriesId`.

| Field | Type | Required | Notes |
| --- | --- | --- | --- |
| `id` | string | Required | Mirrors `seriesId` for backwards compatibility. |
| `seriesId` | string | Required | Stable identifier used across layers. |
| `seriesKey` | string \| null | Optional | Canonical key. |
| `seriesKeyRaw` | string \| null | Optional | Ambiguity trace. |
| `seriesGroupingConfidence` | string | Required | Enum defined above. |
| `episodeIds` | string[] | Required | Ordered membership of the series. |
| `yearFrom` | number \| null | Required | Span start. |
| `yearTo` | number \| null | Required | Span end. |
| `yearConfidence` | string | Required | Confidence score. |
| `fingerprint` | string | Required | Programmatic fingerprint. |
| `memberEpisodeFingerprints` | string[] | Optional | Ordered fingerprints for downstream debugging. |
| `derived` | object | Optional | Deterministic metadata such as counts or subject tags. |
| `seriesTitle` | string | Required | LLM title or fallback. |
| `narrativeSummary` | string \| null | Optional | LLM narrative summary. |
| `tonalDescriptors` | string[] | Optional | LLM tone descriptors. |
| `rssLastSeenAt` | string \| null | Optional | Highest `rssLastSeenAt` among members. |

#### `data/errors.jsonl`
- Append-only JSON Lines ledger capturing recoverable issues.
- Each line is a JSON object with fields:

| Field | Type | Required | Notes |
| --- | --- | --- | --- |
| `stage` | string | Required | Pipeline stage emitting the error (e.g., `llm:episodes`). |
| `itemId` | string | Required | `episodeId` or `seriesId`. |
| `when` | string | Required | ISO 8601 UTC timestamp. |
| `message` | string | Required | Human-readable description. |
| `details` | object \| null | Optional | Structured payload for debugging. |

#### Review console (`/review`)
- Local-only admin screen that surfaces unresolved proposals parsed from `data/errors.jsonl`. The API backing the page (`/api/review/proposals`) filters out any label that already maps to a canonical registry entry or an existing alias/mapping from `data/pending/reviews.jsonl`.
- Moderators can **Accept**, **Reject**, or **Map** any proposal:
  - **Accept** inserts the entity into the appropriate registry file and appends a corresponding record to `data/pending/reviews.jsonl`.
  - **Map** either adds an alias to an existing person/place entry or writes a `topicsMapped` entry (for topics) so future runs auto-resolve the same label.
  - **Reject** logs the decision to `data/pending/reviews.jsonl`.
- The review UI replaces the manual “edit JSON by hand” workflow outlined earlier in this PRD and is the expected way to curate new people/places/topics between pipeline runs.

### 4.3 Formal Schemas
- Authoritative JSON Schemas live alongside the repository under:
  - [`schema/episode.public.schema.json`](../schema/episode.public.schema.json)
  - [`schema/series.public.schema.json`](../schema/series.public.schema.json)
  - [`schema/cache.llm.schema.json`](../schema/cache.llm.schema.json)
- Schemas must include `$schema` and `$id` properties, explicitly type every field above, and declare enum restrictions and `number \| null` variants for year fields.
- All public JSON artefacts (`public/episodes.json`, `public/series.json`) and cache files (`data/episodes-llm.json`, `data/series-llm.json`) MUST validate against the appropriate schema via `scripts/validate.js` using AJV configured with `{ allErrors: true, allowUnionTypes: true }`.
- Provide an `npm run schema:check` script that validates every golden sample file and the current `public/*.json` artefacts against their schemas.
- Schema evolution must remain backwards compatible; coordinate breaking changes with consumer version bumps.

### 4.4 Deterministic Serialization
- Introduce a `stableStringify.ts` helper that serialises JSON objects with keys sorted alphabetically.
- All persisted artefacts must use this helper (directly or via shared utilities) to guarantee byte-for-byte stability across runs.

## 5. Fingerprints & Cleanup Versioning
### 5.1 Episode Fingerprint Formula
The deterministic episode fingerprint MUST incorporate the canonicalised title and Markdown-preserving synopsis:
```
fingerprint = sha256("epfp:v1\ncleanup_v=" + CLEANUP_VERSION + "\n" + cleanTitle + "\n" + cleanDescriptionMarkdown)
```

### 5.2 Series Fingerprint Formula
```
seriesFingerprint = sha256("srfp:v1\n" + seriesId + "\n" + memberEpisodeFingerprints.join("\n"))
```

### 5.3 Programmatic Episode Enrichment
Deterministic cleanup utilities (see §11.3) MUST align with the structured extraction rules below to ensure idempotent outputs regardless of RSS description changes.

#### 5.3.1 Extraction Logic
- **Credit Extraction** — Iterate through the normalised description text (post HTML parsing, pre-boilerplate removal) and search for the following key prefixes. Before matching:
  - Convert all dash and colon variants (`:`, `：`, `﹕`, `꞉`, `-`, `–`, `—`) into a single colon character.
  - Collapse repeated whitespace and trim leading/trailing spaces on each candidate line.
  - Perform case-insensitive prefix matching so that label capitalisation does not matter.
  - Treat pluralised labels (suffix `s` or `es`) as equivalent to their singular form.

  | Prefix String | Target JSON Field |
  | --- | --- |
  | `Producer:` | `credits.producer` |
  | `Producers:` | `credits.producer` (plural alias) |
  | `Senior Producer:` | `credits.seniorProducer` |
  | `Senior Producers:` | `credits.seniorProducer` (plural alias) |
  | `Exec Producer:` | `credits.execProducer` |
  | `Executive Producer:` | `credits.execProducer` |
  | `Exec Producers:` | `credits.execProducer` (plural alias) |
  | `Executive Producers:` | `credits.execProducer` (alias) |
  | `Researcher:` | `credits.researcher` |
  | `Researchers:` | `credits.researcher` (plural alias) |
  | `Assistant Producer:` | `credits.assistantProducer` |
  | `Assistant Producers:` | `credits.assistantProducer` (plural alias) |
  | `Editor:` | `credits.editor` |
  | `Editors:` | `credits.editor` (plural alias) |
  | `Sound Design:` | `credits.soundDesign` |
  | `Sound Designer:` | `credits.soundDesign` (alias) |
  | `Sound Designers:` | `credits.soundDesign` (plural alias) |

  Normalise em/en dashes and colon variants before matching, collapse multiple spaces inside the captured name, and preserve the exact casing from the source for the stored value. Document this alias table in the repository to aid future updates.

- **Boilerplate Removal** — Evaluate each element in `descriptionBlocks` after splitting on double newlines. Drop the entire block if it contains any of the substrings listed below (case-insensitive substring match). Preserve ordering of the remaining blocks.

  - `Learn more about your ad choices`
  - `podcastchoices.com/adchoices`
  - `therestishistory.com`
  - `Join The Rest Is History Club`
  - `The Rest Is History Club`
  - `@TheRestHistory`
  - `instagram.com/theresthistory`
  - `facebook.com/theresthistory`
  - `tiktok.com/@theresthistory`
  - `Get our book`
  - `Buy tickets`
  - `Sign up to our newsletter`

### 5.4 Fingerprint & Hashing Guarantees
- All fingerprint and hash inputs MUST be encoded as UTF-8 bytes before hashing.
- Normalise line endings in every input string to the Unix newline (`\n`) prior to hashing.
- Use the SHA-256 algorithm and emit lower-case hexadecimal digests for all fingerprints.
- String literals embedded in the formulas (e.g., `"epfp:v1\n"`) must not include an implicit trailing newline unless explicitly shown in the formula definition.

### 5.5 Cleanup Version Constant
`CLEANUP_VERSION` is a constant (initially `1`) defined alongside the cleanup helpers. Increment the version whenever deterministic cleanup behaviour changes; doing so invalidates caches intentionally.

### 5.6 LLM Enrichment
LLM enrichment prompts (see §11.5) must remain versioned assets so cached responses can be invalidated deterministically when prompt semantics change.

#### 5.6.1 LLM Prompt Definitions
##### 5.6.1.1 Episode Enrichment Prompt
**System Message / Persona:**

```
You are an expert historical analyst specializing in 'The Rest Is History' podcast. Your task is to extract specific structured metadata from an episode's title and synopsis. You must adhere strictly to the output format.
```

**User Prompt:**

```
Analyze the following podcast episode details:
Title: {{cleanTitle}}
Synopsis: {{cleanDescriptionText}}
From the text provided, perform the following tasks:
Identify key historical figures mentioned. Do NOT include the hosts, Tom Holland and Dominic Sandbrook. Do NOT include producer or staff names mentioned in a credits list.
Identify key geographical places or locations central to the narrative.
Infer a numeric year span (yearFrom, yearTo) for the main historical period discussed. If the episode covers multiple distinct periods or no specific historical period (e.g., mythology, ghosts), you MUST return `null` for both yearFrom and yearTo.
Extract up to five short, key themes or topics that summarize the episode's subject matter.
Select up to three curated key topics from the provided registry. Only emit `topicId` values that exist in the registry. If no topic applies, you may propose ONE new topic that follows the naming rules (Title Case, 1–4 words). Mark proposed topics with `\"isPending\": true` and include a short `\"notes\"` string explaining why the new topic is needed.
Return your analysis ONLY as a single, valid JSON object with the following schema:
```json
{
  "keyPeople": ["string"],
  "keyPlaces": ["string"],
  "keyThemes": ["string"],
  "keyTopics": [
    {
      "id": "string",
      "label": "string",
      "slug": "string",
      "isPending": boolean,
      "notes": "string | null"
    }
  ],
  "yearFrom": number | null,
  "yearTo": number | null
}
```
Example:
Input:
Title: 612. Nelson: The Final Showdown (Part 5)
Synopsis: "After two years at sea chasing the combined fleet of France and Spain, what was Nelson’s next step? Upon returning to his beloved Emma, how was the heroic Nelson received? What was the terrifying Napoleon Bonaparte scheming for his fleet across the seas? And, would Britain finally face an imminent French invasion, and with it apocalypse - for both Britain and Nelson himself? Join Dominic and Tom as they discuss the build up to one of the most totemic naval clashes of all time - Trafalgar - and Nelson; the man behind it all."
Expected Output:
```json
{
  "keyPeople": ["Horatio Nelson", "Emma Hamilton", "Napoleon Bonaparte"],
  "keyPlaces": ["Britain", "France", "Spain", "Trafalgar"],
  "keyThemes": ["Napoleonic Wars", "Naval Warfare", "British Navy", "French Invasion Threat", "Trafalgar Campaign"],
  "keyTopics": [
    { "id": "napoleonic-wars", "label": "Napoleonic Wars", "slug": "napoleonic-wars", "isPending": false }
  ],
  "yearFrom": 1803,
  "yearTo": 1805
}
```
If a span cannot be determined, both `yearFrom` and `yearTo` must be returned as `null`:
```json
{
  "keyPeople": ["Perseus", "Medusa"],
  "keyPlaces": ["Ancient Greece"],
  "keyThemes": ["mythology", "heroic-quests", "monsters"],
  "keyTopics": [],
  "yearFrom": null,
  "yearTo": null
}
```
```

##### 5.6.1.2 Series Enrichment Prompt
**System Message / Persona:**

```
You are a skilled editor tasked with creating a compelling title and summary for a multi-part podcast series based on the titles and synopses of its individual episodes.
```

**User Prompt:**

```
Analyze the following collection of podcast episodes, which belong to a single series:
JSON
{{episodeArray}}
```
*The `episodeArray` will be a JSON array of objects, where each object has `part`, `cleanTitle`, and `cleanDescriptionText`.*

Based on the provided episodes, generate a single, consolidated `seriesTitle` and a short `narrativeSummary` (2-3 sentences) for the entire series. The title should be a human-friendly, overarching name for the arc, derived from the common themes in the episode titles.

Return your analysis ONLY as a single, valid JSON object with the following schema:
```json
{
  "seriesTitle": "string",
  "narrativeSummary": "string"
}
```
```

## 6. Series Key Extraction & Grouping Rules
- Recognise part indicators with flexible patterns: `Part`, `Pt`, `Pt.`, and roman numerals. Matching is case-insensitive and honours the parsing logic in §6.2.
- When ambiguity remains, still assign a `seriesId` but set `seriesGroupingConfidence = "low"` and persist `seriesKeyRaw`.
- Normalise `seriesKey` by trimming whitespace, collapsing repeated separators, and downcasing before passing to `toSlug` (see §22) when generating the slug component of `seriesId`.
- `seriesId` MUST remain stable across reruns and cannot be reused for a different arc.

### 6.1 Arc Boundary Logic
- An arc begins at the first episode in publication order whose title contains a recognised part token where the part number resolves to `1` after parsing.
- Once an arc starts, include subsequent episodes only while:
  - Publication dates remain contiguous, allowing a maximum 14-day gap between episodes, **and**
  - The normalised `seriesKey` matches the starter episode.
- The arc ends immediately when either contiguity breaks or the `seriesKey` changes. If the same `seriesKey` reappears later, it MUST start a new `seriesId` computed via the slugging rules.

### 6.2 Part Indicator Parsing
- Roman numerals in part indicators must be parsed using a strict, case-insensitive regex (e.g., `^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$`).
- Parsed values are capped at `50`; numerals above this threshold are rejected and the episode is treated as lacking a part indicator.
- Convert any roman numerals to Arabic numerals before comparing part indices or applying arc boundary logic.

## 7. Programmatic Cleanup Rules
- Boilerplate markers and credit extraction regexes live in `data/rules/cleanup.yaml`. This YAML file contains:
  - `boilerplateMarkers`: arrays of strings to remove.
  - `creditPatterns`: named regex patterns for credit extraction.
  - `htmlCollapseRules`: directives for collapsing `<br>` spam and redundant tags.
- Topic taxonomy lives in `data/rules/topics.json`. Each entry declares `{ id, label, slug, aliases[], description }` and must follow the naming rules documented in `docs/topics-registry.md`. This file is the source of truth for curated `keyTopics`.
- `stripHtml` must collapse `<br>` spam using the YAML-configured rules.
- Cleanup scripts load rules from YAML at runtime; no hard-coded markers remain in code.
- Maintain unit tests under `tests/cleanup/` covering a representative sample corpus to guard against regressions.

### 7.1 HTML Parsing Implementation
- The reference implementation uses `htmlparser2` with `dom-serializer` in Node.js and must enable full HTML entity decoding.
- Treat `<p>`, `<div>`, and `<li>` as block-level elements that introduce paragraph breaks.
- Interpret `<br>` as a soft line break.
- Strip all other tags entirely, including `<script>` and `<style>` blocks.

## 8. LLM Enrichment & Cache Discipline
- Every pipeline run rewrites JSON artefacts via the stable serializer. Because fingerprints and grouping logic are deterministic, unchanged episodes produce identical records so downstream diffs remain empty.
- LLM cache keys are `${episodeId}:${fingerprint}` or `${seriesId}:${fingerprint}`. Once an item is cached, later runs reuse the stored response; only new episodes or fingerprint changes trigger fresh API calls.
- Programmatic grouping recomputes series metadata on each run, but `seriesId = toSlug(seriesKey) + "-" + firstPubYYYYMMDD` keeps identifiers stable unless the source feed or grouping rules change intentionally.
- Episode cache schema is locked to the minimal shape described in §4.2; extra keys are forbidden.
- Validator behaviour:
  - Trim whitespace on every string, dropping entries shorter than two characters.
  - Before de-duplicating string arrays (e.g., `keyPeople`), canonicalise entries by trimming whitespace, applying Unicode NFKD normalisation, stripping diacritics, and lowercasing for comparison. Preserve the original casing from the first occurrence in the stored array.
  - De-duplicate arrays while preserving original order.
  - Enforce array length constraints and kebab-case for `keyThemes`; ensure every `keyTopics[].id` exists in `data/rules/topics.json` unless `isPending: true`.
- Cache keys remain `${itemId}:${fingerprint}`.
- `data/series-llm.json` may omit `seriesTitle` when status is not `ok`; compose will fall back to programmatic defaults.

### 8.1 LLM Runtime Policy
- Default timeout: 20 seconds per API call.
- Retries: up to 3 attempts with exponential backoff delays of 1s, 2s, then 4s.
- Safety Cap: To prevent accidental, large-scale re-enrichment during routine incremental runs, the pipeline defaults to a maximum of 200 LLM calls per run. This can be configured with the `--max-llm-calls <N>` flag.
- Initial Seeding: For the initial, one-time backfill of the entire episode history, this safety cap must be explicitly overridden. This is done by setting the flag to a higher number or to `-1` for unlimited calls (e.g., `npm run llm:episodes -- --max-llm-calls -1`).
- Logging: record the model name and observed latency for every call.

### 8.2 Cache Management
- LLM caches (`data/episodes-llm.json`, `data/series-llm.json`) are append-only; entries with outdated fingerprints remain but are ignored at read time.
- Provide `scripts/llm/compact-cache.mjs` to remove cache entries whose keys no longer correspond to known fingerprints.
- Schedule a weekly CI job that executes the compaction script to prune stale entries.

## 9. Compose, Redaction & Public Exposure
- Compose precedence is explicit: `publicEpisode = { ...rawEpisode, ...programmaticEpisode, ...llmEpisode }`.
- Series entries apply the same overlay and publish `seriesTitle = llm.seriesTitle || seriesTitleFallback`.
- Fields excluded from the public artefacts:
  - Entire `source` object except `itunesEpisode` (mirrored as a nullable top-level field) and `rssLastSeenAt` (retained for provenance).
  - Internal helper values such as intermediate `yearHints` are not surfaced.
  - LLM metadata (`model`, `promptVersion`, `status`, `notes`) remain internal.
  - `cleanupVersion`, `fingerprint`, and `seriesGroupingConfidence` **are** public for transparency.
- Public field visibility matrix:

| Field | data/episodes | data/series | Public exposure |
| --- | --- | --- | --- |
| `fingerprint` | Programmatic | Programmatic | ✅ (public) |
| `cleanupVersion` | Programmatic | — | ✅ |
| `source.*` | Raw | — | ❌ (`itunesEpisode` exposed via alias) |
| `rssLastSeenAt` | Raw | Derived | ✅ |
| `model/promptVersion/status/notes` | LLM | LLM | ❌ |
| `seriesGroupingConfidence` | Programmatic | Programmatic | ✅ |
| `yearConfidence` | Programmatic/LLM | Programmatic/LLM | ✅ |

### 9.1 Field Precedence Rules
- For `yearFrom`, `yearTo`, and `yearConfidence` in `public/episodes.json`, prefer the values from the LLM cache entry when `status = "ok"` **and** the years are numeric within `[-5000, 2100]`.
- When the preferred LLM values are unavailable or invalid, fall back to the programmatic layer.
- If neither source provides a valid year span, publish `yearFrom = null`, `yearTo = null`, and `yearConfidence = "unknown"`.

## 10. Validation & Contract Checks
- `scripts/validate.js` MUST:
  1. Load schemas via AJV for all public and cache files.
  2. Enforce referential integrity:
     - `episodeId` and `seriesId` uniqueness across all artefacts.
     - All `series.episodeIds[]` exist in `public/episodes.json`.
     - `yearFrom <= yearTo` whenever both values are non-null.
     - When `part` is non-null, `seriesId` MUST be non-null.
  3. Validate data hygiene:
     - Arrays (`keyPeople`, `keyPlaces`, `keyThemes`, `keyTopics`, derived tags) contain no empty strings and are de-duplicated; `keyTopics` also validates registry membership.
     - Enumerated confidence values match the declared enums.
     - Object keys follow the stable ordering helper (fail validation if unsorted serialisation is detected).
  4. RSS provenance checks:
     - If a previously seen `episodeId` reappears with a different `audioUrl`, log an error entry to `data/errors.jsonl` and update the stored value unless the `--lock-audio` flag is passed.
     - Validate that every `source.enclosureUrl` is a well-formed HTTPS URL.

### 10.1 Golden File Testing
- Maintain a `/tests/golden/` directory containing canonical sample outputs for each major JSON artefact (e.g., `episodes-raw.sample.json`, `episodes-programmatic.sample.json`, `public-episodes.sample.json`).
- Add a CI step that runs the pipeline against the bundled sample data and fails if any generated artefact differs from its golden counterpart.
- Pull requests that intentionally change deterministic cleanup or compose logic must refresh the goldens via `npm run test:update-goldens` and commit the updated files.

## 11. Pipeline Flow
```
[RSS Feed]
    ↓
[data/source/rss.YYYY-MM-DD.json]
    ↓
[data/episodes-raw.json]
    ↓
[data/episodes-programmatic.json]
    ↓
[data/series-raw.json]
    ↓
[data/series-programmatic.json]
    ↓
[data/episodes-llm.json]
    ↓
[data/series-llm.json]
    ↓
[public/episodes.json, public/series.json]
    ↓
[validate]
```
Each stage reads the previous layer and only appends or updates the keyed objects for newly discovered items. Non-fatal per-item errors are logged to `data/errors.jsonl`; the pipeline continues unless validation fails.

### 11.1 Fetch RSS
- Script reads from the live feed (`FEED_URL`) and falls back to `data/source/rss.sample.xml` when offline.
- Writes a daily snapshot under `data/source/rss.YYYY-MM-DD.json` with `fetchedAt`.
- Appends unseen episodes into `data/episodes-raw.json` without altering existing entries.

### 11.2 Build Series (Raw)
- Deterministic script analyses episodes to group arcs and writes `data/series-raw.json`.
- Must handle flexible part numbering, shared prefixes, and hyphen/colon variants as described in §6.
- A deterministic override map (`src/config/seriesOverrides.ts`) may inject or amend memberships after heuristic grouping to capture arcs whose titles diverge heavily between parts.

### 11.3 Programmatic Episode Enrichment
- Shared utilities (e.g., `stripHtml`, boilerplate markers) extract clean text once per episode.
- Steps:
  1. Parse HTML → keep only meaningful paragraphs; collapse `<br>` spam using YAML rules.
  2. Identify and drop boilerplate (tour promos, social links, generic "Learn more" lines) using marker lists loaded from `data/rules/cleanup.yaml`.
  3. Extract credits into structured fields (`producer`, `execProducers`, etc.) per YAML regexes.
  4. Normalise whitespace, punctuation, and HTML entities.
  5. Compute fingerprints and persist `cleanupVersion` alongside results.
  6. Detect `Part`/`Pt`/roman numeral suffixes and assign `part`, `seriesId`, and confidence consistently.

### 11.4 Programmatic Series Enrichment
- Input: `data/series-raw.json` + `data/episodes-programmatic.json`.
- For each series:
  - Collect member episode fingerprints to compute the series fingerprint (see §5).
  - Aggregate spans and confidence across members.
  - Derive additional deterministic metadata as needed.

### 11.5 LLM Enrichment
- Episode and series scripts read the programmatic layer, filter for items whose fingerprints lack cached responses, and call OpenAI once per item unless `--force-llm <ID>` is supplied to ignore caches for specific IDs.
- Episode prompts must ignore hosts and boilerplate credits, returning only the minimal schema plus curated `keyTopics` pulled from the registry snapshot supplied in the prompt payload.
- Series prompts may supply titles, summaries, and tonal descriptors; when unavailable or status is not `ok`, compose falls back to programmatic defaults.
- Use the lightweight client in `vendor/openai` and the shared helper for retries/backoff.
- Default to the `gpt-5-nano` model (no temperature parameter supported); fall back to the designated lesser model when the primary is unavailable.
- Errors during enrichment write structured entries to `data/errors.jsonl` without aborting the run.

### 11.6 Compose
- Script merges the three layers to produce `public/*.json` using the precedence rule in §9.
- Series entries apply the same overlay and publish `seriesTitle = llm.seriesTitle || seriesTitleFallback`.
- Programmatic inputs supply `seriesTitleFallback = cleaned seriesKey` (see §13).
- Produces sorted arrays and stable key ordering.

### 11.7 Validate
- Executes the full schema + contract checks described in §10.
- Fails the pipeline when validation errors occur; otherwise succeeds even if LLM stages logged errors.

## 12. Minimal Episode LLM Schema
```
{
  "keyPeople": string[] (max 12, unique, no hosts),
  "keyPlaces": string[] (max 12, unique),
  "keyThemes": string[] (3–8, kebab-case, unique),
  "keyTopics": TopicRef[] (0–3, curated IDs, unique),
  "yearFrom": number \| null,
  "yearTo": number \| null,
  "yearConfidence": "high" | "medium" | "low" | "unknown"
}
```
- Validator trims whitespace, de-duplicates arrays, and drops entries shorter than 2 characters before persistence.
- `TopicRef` is `{ id: string, label: string, slug: string, isPending?: boolean, notes?: string | null }`. Proposed topics MUST set `isPending: true`, emit a short `notes` string in the raw cache entry, and trigger a ledger entry for review.

## 13. Series Title Fallback
- Programmatic fallback `seriesTitleFallback = cleaned seriesKey` ensures compose does not block on LLM availability.
- Public compose exposes `seriesTitle = llm.seriesTitle || seriesTitleFallback` and records `seriesTitleFallback` internally for auditing.

## 14. Error Handling & Ledger
- Non-fatal errors append to `data/errors.jsonl`; the run continues so long as validation passes.
- Validation remains the gatekeeper for exit codes.
- Each ledger entry must include a `level` field constrained to the enum `"info" | "warn" | "error"` to enable filtering.
- Include log entries for LLM errors, cleanup anomalies, and compose fallbacks.
- When the LLM proposes a new topic (`isPending: true`), emit a `level: "info"` ledger entry with `details.topicProposal` so maintainers can approve and append it to `data/rules/topics.json`.
- Rotate `data/errors.jsonl` daily via CI into `data/errors/errors.YYYY-MM-DD.jsonl`, retaining the most recent 30 days by default.

## 15. Validation-Adjacent Guarantees
- Arrays (`keyPeople`, `keyPlaces`, `keyThemes`, `keyTopics`, `episodeIds`, derived tags) must be de-duplicated.
- No empty strings or whitespace-only entries are permitted.
- File size and ordering remain predictable via the stable serialiser.

## 16. CI & Tooling
- Command-line flags:
  - `--dry` — perform a dry run without writes.
  - `--since=YYYY-MM-DD` — limit processing to episodes published on/after the date.
  - `--force-llm <ID>` — bypass caches for a specific `episodeId` or `seriesId`.
  - `--plan` — print a table of items requiring LLM enrichment (new or changed fingerprints) with estimated token cost before issuing API calls.
  - `--output <dir>` — write final artefacts to a temporary directory instead of overwriting in-place outputs.
- GitHub Actions:
  - `ci.yml` runs on push/PR, executes linting, plan-mode pipeline, and tests; upload `public/*.json` + `diff.txt` artefacts for review.
  - `pipeline-publish.yml` runs on schedule/dispatch, executes the full pipeline, commits updated artefacts to `main`, and invokes the Vercel revalidation webhook.
  - Surface `data/errors.jsonl` as an artefact when entries are produced.

## 17. Incremental Behaviour & Migration
- Raw files are append-only; never rewrite old items.
- Programmatic and LLM layers are dictionaries keyed by stable IDs; only new or changed fingerprints trigger updates.
- Provide `scripts/migrate-legacy-caches.mjs` to read legacy caches and emit `episodes-llm.json` / `series-llm.json` entries keyed by the new `{id}:{fingerprint}` format with `promptVersion = "legacy-import"` and `status = "ok"`.
- Migration runs once before the new pipeline executes; document its usage alongside the other scripts.

## 18. Operational Considerations
- **Environment variables**:
  - `OPENAI_API_KEY` must be set locally and as a GitHub repo secret (`secrets.OPENAI_API_KEY`).
  - `OPENAI_MODEL_PRIMARY` defaults to `gpt-5-nano` when unspecified.
  - `OPENAI_MODEL_FALLBACK` defaults to `gpt-4o-mini` and is used whenever the primary model is unavailable.
  - `VERCEL_REVALIDATE_URL` (optional but recommended) triggers cache refresh after the scheduled publish run pushes changes.
- **Model defaults**: scripts must read the environment variables above, choose the first responsive model per call, and log which model handled each API request in accordance with §8.1.
- **Commands** (to be updated in `package.json`):
  ```json
  {
    "scripts": {
      "fetch": "node scripts/fetch-rss.js",
      "build:series": "node scripts/build-series.js",
      "enrich:episodes": "node scripts/enrich/episodes-programmatic.mjs",
      "enrich:series": "node scripts/enrich/series-programmatic.mjs",
      "llm:episodes": "node scripts/llm/episodes.mjs",
      "llm:series": "node scripts/llm/series.mjs",
      "compose": "node scripts/enrich/compose.mjs",
      "validate": "node scripts/validate.js",
      "migrate:caches": "node scripts/migrate-legacy-caches.mjs"
    }
  }
  ```
- **CI sequence**: mirror the command flow above; drop any topic-related steps.

## 19. Success Metrics
- Running the full pipeline with no new episodes should make zero OpenAI requests and leave git clean.
- New episodes should appear end-to-end (raw → public) after a single pipeline execution.
- Manual reruns should be idempotent even if LLM calls fail (compose still produces deterministic outputs from available layers).

## 20. Risks & Mitigations
| Risk | Mitigation |
| --- | --- |
| Fingerprint drift causing unnecessary LLM calls | Keep programmatic cleanup deterministic, version cleanup helpers carefully, and document fingerprint formula. |
| Boilerplate rules accidentally remove genuine content | Maintain a test corpus of sample descriptions and review diffs when rules change. |
| Series membership changes invalidating caches | Fingerprint combines ordered episode fingerprints so membership updates automatically invalidate relevant LLM entries. |
| RSS changes or removals | Track `rssLastSeenAt` and retain snapshots for auditing; provenance allows selective backfills. |

## 21. Open Questions
- Do we need additional metadata (e.g., guest bios) that could be programmatically derived from show notes? (Requires further discovery.)
- Should we expose cleaned Markdown or plain text only? (Decide based on downstream consumer preferences.)
- Are there thresholds for delaying LLM enrichment when OpenAI rate limits occur? (Potential future enhancement.)
- Should we upgrade change detection to treat GUID + enclosure URL mutations as first-class updates? (See §24.)

## 22. Slugging & Normalization Algorithm
- `toSlug(str)` MUST:
  1. Normalise the input string using Unicode NFKD, decomposing accented characters.
  2. Strip all diacritic marks from the decomposed string.
  3. Convert the result to lower-case ASCII.
  4. Replace any sequence of one or more characters that are not `a–z` or `0–9` with a single hyphen (`-`).
  5. Trim leading and trailing hyphens from the final output.
- Example: `toSlug("Napoléon III") === "napoleon-iii"`.
- Use this helper whenever generating the slug portion of a `seriesId` or similar identifiers.

## 23. Security & Data Privacy
- The pipeline processes public RSS content only and must never ingest or store Personally Identifiable Information (PII).
- CI logs and runtime logging must redact API keys and avoid printing full LLM prompts or responses; log only item identifiers, status, and metadata such as latency.
- Treat the guidance here as non-negotiable guardrails for both local runs and CI executions.

## 24. Potential Enhancement — GUID + Enclosure Change Detection

**Status:** Parked (2025-11-04). Track here so the idea is not lost; only schedule work if we encounter recurring freshness issues.

**Problem we’d solve**
- RSS editors sometimes revise audio files or descriptions without changing GUIDs. The current fetcher treats GUID as immutable, so late updates are missed unless we backfill manually.

**Implementation sketch**
- Update `src/pipeline/fetcher.ts` to persist the latest `{ guid, enclosureUrl, rssLastSeenAt }` tuple per item.
- On each fetch, diff against the stored tuple:
  - New GUID → append (existing behaviour).
  - Known GUID with enclosure URL (including query tokens such as `?updated=`) change → record as an update, bump `rssLastSeenAt`, and flag downstream stages as dirty.
  - Optionally maintain a lightweight checksum (e.g., hash of title + description) to catch metadata edits while avoiding churn on trivial ordering changes.
- Propagate updates through enrichment layers:
  - Programmatic layer rewrites the affected entries.
  - Composer merges refreshed metadata and republishes artefacts.
  - Evaluate whether the programmatic fingerprint should incorporate enclosure or description hashes so LLM caches invalidate only when content meaningfully changes.
- Keep publish idempotence: only commit when diffs exist, honour grace windows to prevent thrash, and document the behaviour.

**Pros**
- Keeps published artefacts aligned with corrected audio or text without manual intervention.
- Provides clearer audit trail—updates appear as explicit commits with refreshed timestamps.
- Reduces risk of serving stale audio when the enclosure URL rotates.

**Cons**
- Medium-sized refactor (fetcher, enrichment pipeline, cache invalidation) with regression risk.
- Increased artefact churn and commit noise when the feed makes minor tweaks.
- Potential for unnecessary LLM re-enrichment if fingerprints are mis-scoped, increasing token usage.
- Requires new tests and docs to guarantee determinism and avoid duplicate records.

Revisit when we observe repeated missed updates or commit to broader change-detection improvements.
